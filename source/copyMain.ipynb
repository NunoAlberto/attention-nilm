{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (Houses 2, 3 and 5) -> Houses 4 and 6 do not have microwave data\n",
    "house2 = []\n",
    "microwaveHouse2 = []\n",
    "mainHouse2 = []\n",
    "\n",
    "house3 = []\n",
    "microwaveHouse3 = []\n",
    "mainHouse3 = []\n",
    "\n",
    "house5 = []\n",
    "microwaveHouse5 = []\n",
    "mainHouse5 = []\n",
    "\n",
    "for i in range(2,7):\n",
    "    house = \"../redd-preprocessed/redd_house{}_\".format(i)\n",
    "    if i == 2:\n",
    "        for ii in range(0,7):\n",
    "            house2.append(np.array(pd.read_csv(house + \"{}.csv\".format(ii))).tolist())\n",
    "        for file in house2:\n",
    "            for timestep in file:\n",
    "                microwaveHouse2.append(float(timestep[4]))\n",
    "                mainHouse2.append(float(timestep[7]))\n",
    "    elif i == 3:\n",
    "        for ii in range(0,6):\n",
    "            house3.append(np.array(pd.read_csv(house + \"{}.csv\".format(ii))).tolist())\n",
    "        for file in house3:\n",
    "            for timestep in file:\n",
    "                microwaveHouse3.append(float(timestep[5]))\n",
    "                mainHouse3.append(float(timestep[8]))\n",
    "    elif i == 5:\n",
    "        for timestep in np.array(pd.read_csv(house + \"0.csv\")).tolist():\n",
    "            microwaveHouse5.append(float(timestep[6]))\n",
    "            mainHouse5.append(float(timestep[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House 2 first Microwave datapoint: 5.0\n",
      "House 2 first Main datapoint: 272.27999877900004\n",
      "House 2 last Microwave datapoint: 5.0\n",
      "House 2 last Main datapoint: 291.76998901400003\n",
      "House 3 first Microwave datapoint: 2.0\n",
      "House 3 first Main datapoint: 258.070007324\n",
      "House 3 last Microwave datapoint: 2.0\n",
      "House 3 last Main datapoint: 114.580001831\n",
      "House 5 first Microwave datapoint: 4.0\n",
      "House 5 first Main datapoint: 6077.21972656\n",
      "House 5 last Microwave datapoint: 3.0\n",
      "House 5 last Main datapoint: 184.92999267599998\n"
     ]
    }
   ],
   "source": [
    "print(\"House 2 first Microwave datapoint: \" + str(np.array(microwaveHouse2)[0]))\n",
    "print(\"House 2 first Main datapoint: \" + str(np.array(mainHouse2)[0]))\n",
    "print(\"House 2 last Microwave datapoint: \" + str(np.array(microwaveHouse2)[len(microwaveHouse2)-1]))\n",
    "print(\"House 2 last Main datapoint: \" + str(np.array(mainHouse2)[len(mainHouse2)-1]))\n",
    "\n",
    "print(\"House 3 first Microwave datapoint: \" + str(np.array(microwaveHouse3)[0]))\n",
    "print(\"House 3 first Main datapoint: \" + str(np.array(mainHouse3)[0]))\n",
    "print(\"House 3 last Microwave datapoint: \" + str(np.array(microwaveHouse3)[len(microwaveHouse3)-1]))\n",
    "print(\"House 3 last Main datapoint: \" + str(np.array(mainHouse3)[len(mainHouse3)-1]))\n",
    "\n",
    "print(\"House 5 first Microwave datapoint: \" + str(np.array(microwaveHouse5)[0]))\n",
    "print(\"House 5 first Main datapoint: \" + str(np.array(mainHouse5)[0]))\n",
    "print(\"House 5 last Microwave datapoint: \" + str(np.array(microwaveHouse5)[len(microwaveHouse5)-1]))\n",
    "print(\"House 5 last Main datapoint: \" + str(np.array(mainHouse5)[len(mainHouse5)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data (House 1)\n",
    "house1 = []\n",
    "microwaveHouse1 = []\n",
    "mainHouse1 = []\n",
    "\n",
    "for ii in range(0,11):\n",
    "    house1.append(np.array(pd.read_csv(\"../redd-preprocessed/redd_house1_\" + \"{}.csv\".format(ii))).tolist())\n",
    "\n",
    "for file in house1:\n",
    "    for timestep in file:\n",
    "        try:\n",
    "            int(timestep[5])\n",
    "            int(timestep[7])\n",
    "        except:\n",
    "            # Removes a single nan value in the Main channel\n",
    "            continue\n",
    "        microwaveHouse1.append(float(timestep[5]))\n",
    "        mainHouse1.append(float(timestep[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House 1 first Microwave datapoint: 4.0\n",
      "House 1 first Main datapoint: 103.790000916\n",
      "House 1 last Microwave datapoint: 4.0\n",
      "House 1 last Main datapoint: 278.160003662\n"
     ]
    }
   ],
   "source": [
    "print(\"House 1 first Microwave datapoint: \" + str(np.array(microwaveHouse1)[0]))\n",
    "print(\"House 1 first Main datapoint: \" + str(np.array(mainHouse1)[0]))\n",
    "print(\"House 1 last Microwave datapoint: \" + str(np.array(microwaveHouse1)[len(microwaveHouse1)-1]))\n",
    "print(\"House 1 last Main datapoint: \" + str(np.array(mainHouse1)[len(mainHouse1)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "applianceTrainingData = microwaveHouse2\n",
    "mainTrainingData = mainHouse2\n",
    "\n",
    "applianceTrainingData.extend(microwaveHouse3)\n",
    "mainTrainingData.extend(mainHouse3)\n",
    "\n",
    "applianceTrainingData.extend(microwaveHouse5)\n",
    "mainTrainingData.extend(mainHouse5)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(mainTrainingData, applianceTrainingData, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, mains, appliances_regression, appliances_classification, window_size, batch_size, shuffle=False):\n",
    "        self.mains = mains\n",
    "        self.appliances_regression = appliances_regression\n",
    "        self.appliances_classification = appliances_classification\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.mains) - self.window_size + 1)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.ceil(len(self.indices) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mains_batch = []\n",
    "        appliances_regression_batch = []\n",
    "        appliances_classification_batch = []\n",
    "        appliance_regression_sample = []\n",
    "        appliance_classification_sample = []\n",
    "\n",
    "        if idx == self.__len__() - 1:\n",
    "            inds = self.indices[idx * self.batch_size:]\n",
    "        else:\n",
    "            inds = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "\n",
    "        for i in inds:\n",
    "            main_sample = self.mains[i:i + self.window_size]\n",
    "            appliance_regression_sample = self.appliances_regression[i:i + self.window_size]\n",
    "            appliance_classification_sample = self.appliances_classification[i:i + self.window_size]\n",
    "\n",
    "            mains_batch.append(main_sample)\n",
    "            appliances_regression_batch.append(appliance_regression_sample)\n",
    "            appliances_classification_batch.append(appliance_classification_sample)\n",
    "\n",
    "        mains_batch_np = np.array(mains_batch)\n",
    "        mains_batch_np = np.reshape(mains_batch_np, (mains_batch_np.shape[0], mains_batch_np.shape[1], 1))\n",
    "        appliances_regression_batch_np = np.array(appliances_regression_batch)\n",
    "        appliances_regression_batch_np = np.reshape(appliances_regression_batch_np,\n",
    "                                                    (appliances_regression_batch_np.shape[0],\n",
    "                                                     appliances_regression_batch_np.shape[1]))\n",
    "        appliances_classification_batch_np = np.array(appliances_classification_batch)\n",
    "        appliances_classification_batch_np = np.reshape(appliances_classification_batch_np,\n",
    "                                                        (appliances_classification_batch_np.shape[0],\n",
    "                                                         appliances_classification_batch_np.shape[1]))\n",
    "        return mains_batch_np, [appliances_regression_batch_np, appliances_classification_batch_np]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        weight_initializer = 'he_normal'\n",
    "        self.W = tf.keras.layers.Dense(units, kernel_initializer=weight_initializer)\n",
    "        self.V = tf.keras.layers.Dense(1, kernel_initializer=weight_initializer)\n",
    "\n",
    "    def call(self, encoder_output, **kwargs):\n",
    "        # encoder_output shape == (batch_size, seq_length, latent_dim)\n",
    "        # score shape == (batch_size, seq_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, seq_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W(encoder_output)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, seq_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window_size, filters, kernel_size, units):\n",
    "    input_data = tf.keras.Input(shape=(window_size, 1))\n",
    "\n",
    "    # CLASSIFICATION SUBNETWORK\n",
    "    x = tf.keras.layers.Conv1D(filters=30, kernel_size=10, activation='relu')(input_data)\n",
    "    x = tf.keras.layers.Conv1D(filters=30, kernel_size=8, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=40, kernel_size=6, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(units=1024, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    classification_output = tf.keras.layers.Dense(units=window_size, activation='sigmoid', name=\"classification_output\")(x)\n",
    "\n",
    "    #REGRESSION SUBNETWORK\n",
    "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(input_data)\n",
    "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
    "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
    "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
    "    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, activation=\"tanh\", return_sequences=True), merge_mode=\"concat\")(y)\n",
    "    y, weights = AttentionLayer(units=units)(y)\n",
    "    y = tf.keras.layers.Dense(units, activation='relu')(y)\n",
    "    regression_output = tf.keras.layers.Dense(window_size, activation='relu', name=\"regression_output\")(y)\n",
    "\n",
    "    output = tf.keras.layers.Multiply(name=\"output\")([regression_output, classification_output])\n",
    "\n",
    "    full_model = tf.keras.Model(inputs=input_data, outputs=[output, classification_output], name=\"LDwA\")\n",
    "    attention_model = tf.keras.Model(inputs=input_data, outputs=weights)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    full_model.compile(optimizer=optimizer, loss={\n",
    "        \"output\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"classification_output\": tf.keras.losses.BinaryCrossentropy()})\n",
    "\n",
    "    return full_model, attention_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(prediction, true):\n",
    "    MAE = abs(true - prediction)\n",
    "    MAE = np.sum(MAE)\n",
    "    MAE = MAE / len(prediction)\n",
    "    return MAE\n",
    "\n",
    "\n",
    "def sae(prediction, true, N):\n",
    "    T = len(prediction)\n",
    "    K = int(T / N)\n",
    "    SAE = 0\n",
    "    for k in range(1, N):\n",
    "        pred_r = np.sum(prediction[k * N: (k + 1) * N])\n",
    "        true_r = np.sum(true[k * N: (k + 1) * N])\n",
    "        SAE += abs(true_r - pred_r)\n",
    "    SAE = SAE / (K * N)\n",
    "    return SAE\n",
    "\n",
    "\n",
    "def f1(prediction, true):\n",
    "    epsilon = 1e-8\n",
    "    TP = epsilon\n",
    "    FN = epsilon\n",
    "    FP = epsilon\n",
    "    TN = epsilon\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] >= 0.5:\n",
    "            prediction_binary = 1\n",
    "        else:\n",
    "            prediction_binary = 0\n",
    "        if prediction_binary == 1 and true[i] == 1:\n",
    "            TP += 1\n",
    "        elif prediction_binary == 0 and true[i] == 1:\n",
    "            FN += 1\n",
    "        elif prediction_binary == 1 and true[i] == 0:\n",
    "            FP += 1\n",
    "        elif prediction_binary == 0 and true[i] == 0:\n",
    "            TN += 1\n",
    "    R = TP / (TP + FN)\n",
    "    P = TP / (TP + FP)\n",
    "    f1 = (2 * P * R) / (P + R)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def standardize_data(data, mu=0.0, sigma=1.0):\n",
    "    #changed from (data -= mu) & (data /= sigma) due to errors in the cell below\n",
    "    data = data - mu\n",
    "    data = data / sigma\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_data(data, min_value=0.0, max_value=1.0):\n",
    "    #changed from (data -= min_value) & (data /= max_value - min_value) due to errors in the cell below\n",
    "    data = data - min_value\n",
    "    data = data / (max_value - min_value)\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_overall_sequence(sequences):\n",
    "    unique_sequence = []\n",
    "    matrix = [sequences[::-1, :].diagonal(i) for i in range(-sequences.shape[0] + 1, sequences.shape[1])]\n",
    "    for i in range(len(matrix)):\n",
    "        unique_sequence.append(np.median(matrix[i]))\n",
    "    unique_sequence = np.array(unique_sequence)\n",
    "    return unique_sequence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Training only with House 5 data\n",
    "X_train, X_val, y_train, y_val  = train_test_split(mainHouse5, microwaveHouse5, test_size=0.2, random_state=0)\n",
    "\n",
    "# Read the NILM dataset\n",
    "main_train, appliance_train = np.array(X_train), np.array(y_train)\n",
    "main_val, appliance_val = np.array(X_val), np.array(y_val)\n",
    "main_test, appliance_test = np.array(mainHouse1), np.array(microwaveHouse1)\n",
    "\n",
    "# Choose the appliance-specific window size\n",
    "window_size = 128\n",
    "\n",
    "# Threshold of 15 Watt for detecting the ON/OFF states\n",
    "THRESHOLD = 15\n",
    "\n",
    "# Build ON/OFF appliance vector for the classification subtask\n",
    "appliance_train_classification = np.copy(appliance_train)\n",
    "appliance_train_classification[appliance_train_classification <= THRESHOLD] = 0\n",
    "appliance_train_classification[appliance_train_classification > THRESHOLD] = 1\n",
    "\n",
    "appliance_val_classification = np.copy(appliance_val)\n",
    "appliance_val_classification[appliance_val_classification <= THRESHOLD] = 0\n",
    "appliance_val_classification[appliance_val_classification > THRESHOLD] = 1\n",
    "\n",
    "# Standardization of the main power and normalization of appliance power\n",
    "appliance_min_power = np.min(appliance_train)\n",
    "appliance_max_power = np.max(appliance_train)\n",
    "main_std = np.std(main_train)\n",
    "main_mean = np.mean(main_train)\n",
    "\n",
    "main_train = standardize_data(main_train, np.mean(main_train), np.std(main_train))\n",
    "main_val = standardize_data(main_val, np.mean(main_val), np.std(main_val))\n",
    "\n",
    "appliance_train_regression = np.copy(appliance_train)\n",
    "appliance_train_regression = normalize_data(appliance_train_regression, appliance_min_power, appliance_max_power)\n",
    "\n",
    "appliance_val_regression = np.copy(appliance_val)\n",
    "appliance_val_regression = normalize_data(appliance_val_regression, appliance_min_power, appliance_max_power)\n",
    "\n",
    "# DataGenerator -> def __init__(self, mains, appliances_regression, appliances_classification, window_size, batch_size, shuffle=False):\n",
    "\n",
    "# Dataset generator\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(main_train, appliance_train_regression,\n",
    "                                appliance_train_classification, window_size, batch_size)\n",
    "val_generator = DataGenerator(main_val, appliance_val_regression,\n",
    "                                appliance_val_classification, window_size, batch_size)\n",
    "\n",
    "train_steps = train_generator.__len__()\n",
    "validation_steps = val_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m train_generator:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOK\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(train_generator))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if train_generator:\n",
    "    print(\"OK\")\n",
    "print(len(train_generator))\n",
    "x, y = train_generator[0]\n",
    "print(y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LDwA\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 128, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 119, 30)      330         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 125, 32)      160         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 112, 30)      7230        ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 122, 32)      4128        ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 107, 40)      7240        ['conv1d_21[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 119, 32)      4128        ['conv1d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 103, 50)      10050       ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 116, 32)      4128        ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 99, 50)       12550       ['conv1d_23[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 116, 256)    164864      ['conv1d_29[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 95, 50)       12550       ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " attention_layer_2 (AttentionLa  ((None, 256),       33025       ['bidirectional_2[0][0]']        \n",
      " yer)                            (None, 116, 1))                                                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 4750)         0           ['conv1d_25[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 128)          32896       ['attention_layer_2[0][0]']      \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1024)         4865024     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " regression_output (Dense)      (None, 128)          16512       ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " classification_output (Dense)  (None, 128)          131200      ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " output (Multiply)              (None, 128)          0           ['regression_output[0][0]',      \n",
      "                                                                  'classification_output[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,306,015\n",
      "Trainable params: 5,306,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tune the appliance-dependent parameters\n",
    "filters = 32\n",
    "kernel_size = 4\n",
    "units = 128\n",
    "\n",
    "model, att_model = build_model(window_size, filters, kernel_size, units)\n",
    "model.summary()\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Original training method\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, steps_per_epoch\u001b[39m=\u001b[39;49mtrain_steps,\n\u001b[1;32m      3\u001b[0m                         validation_data\u001b[39m=\u001b[39;49mval_generator, validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m      4\u001b[0m                         callbacks\u001b[39m=\u001b[39;49m[early_stop], verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nilmtk-env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nilmtk-env/lib/python3.8/site-packages/keras/engine/training.py:1155\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[39mif\u001b[39;00m validation_split:\n\u001b[1;32m   1149\u001b[0m   \u001b[39m# Create the validation data using the training data. Only supported for\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m   \u001b[39m# `Tensor` and `NumPy` input.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m   (x, y, sample_weight), validation_data \u001b[39m=\u001b[39m (\n\u001b[1;32m   1152\u001b[0m       data_adapter\u001b[39m.\u001b[39mtrain_validation_split(\n\u001b[1;32m   1153\u001b[0m           (x, y, sample_weight), validation_split\u001b[39m=\u001b[39mvalidation_split))\n\u001b[0;32m-> 1155\u001b[0m \u001b[39mif\u001b[39;00m validation_data:\n\u001b[1;32m   1156\u001b[0m   val_x, val_y, val_sample_weight \u001b[39m=\u001b[39m (\n\u001b[1;32m   1157\u001b[0m       data_adapter\u001b[39m.\u001b[39munpack_x_y_sample_weight(validation_data))\n\u001b[1;32m   1159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39m_should_use_with_coordinator:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Original training method\n",
    "history = model.fit(x=train_generator, epochs=100, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_generator, validation_steps=validation_steps,\n",
    "                        callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sequence length = 558288\n",
      "        windows length = 128\n",
      "        stride = 1\n",
      "        segments = 558161\n",
      "(558161, 128) (558161, 128)\n",
      "        sequence length = 242044\n",
      "        windows length = 128\n",
      "        stride = 1\n",
      "        segments = 241917\n",
      "(241917, 128) (241917, 128)\n",
      "        sequence length = 24181\n",
      "        windows length = 128\n",
      "        stride = 1\n",
      "        segments = 24054\n",
      "(24054, 128) (24054, 128)\n"
     ]
    }
   ],
   "source": [
    "def shift_segment(X,y,seg_length,stride,print_info=True):\n",
    "    \n",
    "    X_o_seg = []\n",
    "    y_o_seg = []\n",
    "      \n",
    "    for i in range(len(X)-seg_length+1):\n",
    "        if i%stride==0:\n",
    "            assert len(X[i:i+seg_length]) == seg_length\n",
    "            X_o_seg.append(  X[i:i+seg_length].reshape(-1) ) \n",
    "\n",
    "            #y_o_seg.append(  y[i+seg_length//2-1, 0] )\n",
    "            y_o_seg.append(  y[i:i+seg_length, 0] )\n",
    "    if print_info==True:           \n",
    "        print(' '*7,'sequence length = {}'.format(len(X)))\n",
    "        print(' '*7,'windows length = {}'.format(seg_length))\n",
    "        print(' '*7,'stride = {}'.format(stride))\n",
    "        print(' '*7,'segments =',len(y_o_seg))\n",
    "    \n",
    "    return np.array(X_o_seg), np.array(y_o_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LDwA\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 128, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_350 (Conv1D)            (None, 119, 30)      330         ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_356 (Conv1D)            (None, 125, 32)      160         ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_351 (Conv1D)            (None, 112, 30)      7230        ['conv1d_350[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_357 (Conv1D)            (None, 122, 32)      4128        ['conv1d_356[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_352 (Conv1D)            (None, 107, 40)      7240        ['conv1d_351[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_358 (Conv1D)            (None, 119, 32)      4128        ['conv1d_357[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_353 (Conv1D)            (None, 103, 50)      10050       ['conv1d_352[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_359 (Conv1D)            (None, 116, 32)      4128        ['conv1d_358[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_354 (Conv1D)            (None, 99, 50)       12550       ['conv1d_353[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_35 (Bidirectiona  (None, 116, 256)    164864      ['conv1d_359[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_355 (Conv1D)            (None, 95, 50)       12550       ['conv1d_354[0][0]']             \n",
      "                                                                                                  \n",
      " attention_layer_35 (AttentionL  ((None, 256),       33025       ['bidirectional_35[0][0]']       \n",
      " ayer)                           (None, 116, 1))                                                  \n",
      "                                                                                                  \n",
      " flatten_35 (Flatten)           (None, 4750)         0           ['conv1d_355[0][0]']             \n",
      "                                                                                                  \n",
      " dense_143 (Dense)              (None, 128)          32896       ['attention_layer_35[0][0]']     \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 1024)         4865024     ['flatten_35[0][0]']             \n",
      "                                                                                                  \n",
      " regression_output (Dense)      (None, 128)          16512       ['dense_143[0][0]']              \n",
      "                                                                                                  \n",
      " classification_output (Dense)  (None, 128)          131200      ['dense_140[0][0]']              \n",
      "                                                                                                  \n",
      " output (Multiply)              (None, 128)          0           ['regression_output[0][0]',      \n",
      "                                                                  'classification_output[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,306,015\n",
      "Trainable params: 5,306,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isidentifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[474], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m#print(len(int(train_generator)))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m\"\"\"history = model.fit(x=train_generator, epochs=100, steps_per_epoch=train_steps,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m                    validation_data=val_generator, validation_steps=validation_steps,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m                    callbacks=[early_stop], verbose=1)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dataSetTraining \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensors(X_train, y_train)\n\u001b[1;32m     17\u001b[0m dataSetValidation \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensors(X_val, y_val)\n\u001b[1;32m     19\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39mdataSetTraining, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, steps_per_epoch\u001b[39m=\u001b[39mtrain_steps,\n\u001b[1;32m     20\u001b[0m                     validation_data\u001b[39m=\u001b[39mdataSetValidation, validation_steps\u001b[39m=\u001b[39mvalidation_steps,\n\u001b[1;32m     21\u001b[0m                     callbacks\u001b[39m=\u001b[39m[early_stop], verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nilmtk-env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:701\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensors\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    666\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \n\u001b[1;32m    668\u001b[0m \u001b[39m  `from_tensors` produces a dataset containing only a single element. To slice\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nilmtk-env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4641\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   4639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m dataset_metadata_pb2\u001b[39m.\u001b[39mMetadata()\n\u001b[1;32m   4640\u001b[0m \u001b[39mif\u001b[39;00m name:\n\u001b[0;32m-> 4641\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m _validate_and_encode(name)\n\u001b[1;32m   4642\u001b[0m kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   4643\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mor\u001b[39;00m compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2021\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m30\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nilmtk-env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:118\u001b[0m, in \u001b[0;36m_validate_and_encode\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_and_encode\u001b[39m(name):\n\u001b[0;32m--> 118\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m name\u001b[39m.\u001b[39;49misidentifier():\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid `name`. The argument `name` needs to be a valid \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39midentifier. Value is considered a valid identifier if it \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39monly contains alphanumeric characters (a-z), (A-Z), and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m(0-9), or underscores (_). A valid identifier cannot \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mstart with a number, or contain any spaces.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m   \u001b[39mreturn\u001b[39;00m name\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isidentifier'"
     ]
    }
   ],
   "source": [
    "# the amount of shifting from one window training example to the next\n",
    "stride = 1\n",
    "\n",
    "first = True\n",
    "\n",
    "for main,appliance in [(mainHouse2, microwaveHouse2),(mainHouse3, microwaveHouse3),(mainHouse5, microwaveHouse5)]:\n",
    "\n",
    "    # Standardization of the main power and normalization of appliance power\n",
    "    main = standardize_data(main, np.mean(main), np.std(main))\n",
    "\n",
    "    appliance_min_power = np.min(appliance)\n",
    "    appliance_max_power = np.max(appliance)\n",
    "    appliance_regression = np.copy(appliance)\n",
    "    appliance_regression = normalize_data(appliance_regression, appliance_min_power, appliance_max_power)\n",
    "    \n",
    "    #X_i,y_i = expand_dim_data(X, y, MAX_X, MAX_y)\n",
    "    X = np.expand_dims(main, 1) \n",
    "    y = np.expand_dims(appliance_regression, 1) \n",
    "    X_seg_i, y_seg_i = shift_segment(X,y,window_size,stride)\n",
    "    \n",
    "    print(X_seg_i.shape, y_seg_i.shape)\n",
    "    # Combine data from all houses\n",
    "    if first == True:\n",
    "\n",
    "        X_train = main.tolist()\n",
    "        y_train = appliance_regression.tolist()\n",
    "        \n",
    "        X_o_train_seg = X_seg_i.tolist()\n",
    "        y_o_train_seg = y_seg_i.tolist()\n",
    "\n",
    "        first = False\n",
    "    else:\n",
    "        \n",
    "        \"\"\"X_train = np.vstack((X_train,main))\n",
    "        y_train = np.vstack((y_train,appliance_regression))\n",
    "\n",
    "        X_o_train_seg = np.vstack((X_o_train_seg,X_seg_i))\n",
    "        y_o_train_seg = np.hstack((y_o_train_seg,y_seg_i))\"\"\"\n",
    "\n",
    "        X_train.extend(main)\n",
    "        y_train.extend(appliance_regression)\n",
    "\n",
    "        X_o_train_seg.extend(X_seg_i)\n",
    "        y_o_train_seg.extend(y_seg_i)\n",
    "\n",
    "#X_train, X_val, y_train, y_val  = train_test_split(X_o_train_seg, y_o_train_seg, test_size=0.2, random_state=0)\n",
    "\n",
    "train_steps = X_train.__len__()\n",
    "validation_steps = X_val.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 1 training method\n",
    "history = model.fit(x=X_o_train_seg, y=y_o_train_seg, epochs=100,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 2 training method\n",
    "dataSetTraining = tf.data.Dataset.from_tensors(X_train, y_train)\n",
    "dataSetValidation = tf.data.Dataset.from_tensors(X_val, y_val)\n",
    "\n",
    "history = model.fit(x=dataSetTraining, epochs=100, steps_per_epoch=train_steps,\n",
    "                    validation_data=dataSetValidation, validation_steps=validation_steps,\n",
    "                    callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 3 training method\n",
    "history = model.fit(x=X_train, y=y_train, epochs=100, steps_per_epoch=train_steps,\n",
    "                    validation_data=(X_val, y_val), validation_steps=validation_steps,\n",
    "                    callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[393], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Plotting the results of training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history_dict \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mLoss during training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(history\u001b[39m.\u001b[39mepoch)), history_dict[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting the results of training\n",
    "history_dict = history.history\n",
    "plt.title('Loss during training')\n",
    "plt.plot(np.arange(len(history.epoch)), history_dict['loss'])\n",
    "plt.plot(np.arange(len(history.epoch)), history_dict['val_loss'])\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "appliance_test_classification = np.copy(appliance_test)\n",
    "appliance_test_classification[appliance_test_classification <= THRESHOLD] = 0\n",
    "appliance_test_classification[appliance_test_classification > THRESHOLD] = 1\n",
    "\n",
    "appliance_min_power = np.min(appliance_train)\n",
    "appliance_max_power = np.max(appliance_train)\n",
    "\n",
    "main_test = standardize_data(main_test, np.mean(main_test), np.std(main_test))\n",
    "\n",
    "appliance_test_regression = np.copy(appliance_test)\n",
    "appliance_test_regression = normalize_data(appliance_test_regression, appliance_min_power, appliance_max_power)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_generator = DataGenerator(main_test, appliance_test_regression,\n",
    "                        appliance_test_classification, window_size, batch_size)\n",
    "\n",
    "test_steps = test_generator.__len__()\n",
    "\n",
    "results = model.evaluate(x=test_generator, steps=test_steps)\n",
    "predicted_output, predicted_on_off = model.predict(x=test_generator, steps=test_steps)\n",
    "\n",
    "predicted_output *= (appliance_max_power - appliance_min_power)\n",
    "predicted_output += appliance_min_power\n",
    "# Clip negative values to zero\n",
    "predicted_output[predicted_output < 0] = 0.0\n",
    "\n",
    "prediction = build_overall_sequence(predicted_output)\n",
    "prediction_on_off = build_overall_sequence(predicted_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "N = 1200\n",
    "MAE = mae(prediction, appliance_test)\n",
    "SAE = sae(prediction, appliance_test, N=N)\n",
    "F1 = f1(prediction_on_off, appliance_test_classification)\n",
    "\n",
    "print(\"MAE = {}\".format(MAE))\n",
    "print(\"SAE = {}\".format(SAE))\n",
    "print(\"F1 = {}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result of the prediction\n",
    "fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(50, 40))\n",
    "axes[0].set_title(\"Real\")\n",
    "axes[0].plot(np.arange(len(appliance_test)), appliance_test, color='blue')\n",
    "axes[1].set_title(\"Prediction\")\n",
    "axes[1].plot(np.arange(len(prediction)), prediction, color='orange')\n",
    "axes[2].set_title(\"Real vs prediction\")\n",
    "axes[2].plot(np.arange(len(appliance_test)), appliance_test, color='blue')\n",
    "axes[2].plot(np.arange(len(prediction)), prediction, color='orange')\n",
    "axes[3].set_title(\"Real on off\")\n",
    "axes[3].plot(np.arange(len(appliance_test_classification)), appliance_test_classification, color='blue')\n",
    "axes[4].set_title(\"Prediction on off\")\n",
    "axes[4].plot(np.arange(len(prediction_on_off)), prediction_on_off, color='orange')\n",
    "axes[5].set_title(\"Real vs Prediction on off\")\n",
    "axes[5].plot(np.arange(len(appliance_test_classification)), appliance_test_classification, color='blue')\n",
    "axes[5].plot(np.arange(len(prediction_on_off)), prediction_on_off, color='orange')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa8f9c1dccc55248fad0a084e9ed820bc3954390e5eb9944ee580a12eb54c085"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
